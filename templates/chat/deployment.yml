# Deployment configuration for llama-deploy
#
# This file defines your deployment setup including:
# - Control plane configuration
# - Message queue settings
# - Services (workflows and UI components)
#
# For more information, see: https://github.com/run-llama/llama-deploy

name: my-fancy-app

# Control plane configuration
# The control plane manages the state of the system and coordinates services
control-plane:
  services_store_key: services
  tasks_store_key: tasks
  session_store_key: sessions
  step_interval: 0.1
  host: 127.0.0.1
  port: 8000
  internal_host: null
  internal_port: null
  running: true
  cors_origins: null
  topic_namespace: llama_deploy
  state_store_uri: null
  use_tls: false

# Message queue configuration
# The message queue handles communication between services
message-queue:
  type: simple
  host: 127.0.0.1
  port: 8001
  client_kwargs: {}
  raise_exceptions: false
  use_ssl: false

# The default service to use when no service is specified
default-service: example_workflow

# Service definitions
# Each service represents a workflow or component in your system
services:
  example_workflow:
    name: Example Workflow
    source:
      type: local
      location: src
    import-path: src/workflow:workflow
    host: null
    port: null
    env:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    env-files:
    - ./.env
    python-dependencies:
    - llama-index-core>=0.12.37
    - llama-index-llms-openai
    ts-dependencies: null

# UI component configuration
# This defines the web interface for your deployment
ui:
  name: Example UI
  source:
    type: local
    location: .
  import-path: ui
  host: null
  port: null
  env: null
  env-files: null
  python-dependencies: null
  ts-dependencies: {}
