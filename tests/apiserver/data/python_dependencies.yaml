name: MyDeployment

control-plane:
  port: 8000

message-queue:
  type: simple
  host: "127.0.0.1"
  port: 8001

default-service: myworkflow

services:
  myworkflow:
    name: My Python Workflow
    python-dependencies:
      - "llama-index-core<1"
      - "llama-index-llms-openai"
    source:
      type: local
      location: test
