name: MyDeployment

control-plane:
  port: 8000

message-queue:
  type: simple
  host: "127.0.0.1"
  port: 8001

default-service: myworkflow

services:
  myworkflow:
    # A python workflow available in a git repo
    name: My Python Workflow
    source:
      type: git
      name: git@github.com/myorg/myrepo
    path: src/python/app # relative to root of the repo
    port: 1313
    python-dependencies:
      # this is a list to match the requirements.txt format
      - "llama-index-core<1"
      - "llama-index-llms-openai"
      # we can also support installing a req file relative to `path`
      # if source is a git repository
      - "requirements.txt"
    env:
      VAR_1: x
      VAR_2: y
    env-files:
      - ./.env

  another-workflow:
    # A LITS workflow available in a git repo (might be the same)
    name: My LITS Workflow
    source:
      type: git
      name: git@github.com/myorg/myrepo
    path: src/ts/app
    port: 1313
    ts-dependencies:
      # this is a mapping to match the package.json format
      "@llamaindex/core": "^0.2.0"
      "@notionhq/client": "^2.2.15"

  dockerservice:
    # A rust binary running in a container
    name: My additional service
    source:
      type: docker
      name: myorg/myimage:latest
    port: 1313

  memory:
    name: Chat Memory
