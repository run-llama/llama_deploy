{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb9bb70-5753-46b8-93eb-ab7cc9676a4f",
   "metadata": {},
   "source": [
    "# Corrective RAG as a Service\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this guide we show you how to use llama-agents to build [CRAG (Corrective RAG)](https://arxiv.org/abs/2401.15884) by Yan et al. as a service.\n",
    "\n",
    "![CRAG Diagram](assets/corrective_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feed1d7-cdf1-45d8-9fd1-0192179cb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index tavily-python llama-index-tools-tavily-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baa699-5e0e-4268-8601-d4261cb5e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eab183-6bbd-40d6-8cde-dade2b3d778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8067f-caa8-4cf3-a000-cbae55ad9fe2",
   "metadata": {},
   "source": [
    "## Setup Data, Indexes, and Tools\n",
    "\n",
    "We do the following:\n",
    "- Download the Gemini paper as an example document to build RAG over. **NOTE**: We use LlamaParse to parse the document which requires a [LlamaCloud account](https://cloud.llamaindex.ai/). If you choose to use a purely open-source reader, you can do that too.\n",
    "- Setup a vector index over this paper\n",
    "- Setup a web search tool (powered by Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83ccd0-873d-401f-86bb-513c0e61f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 26.2M  100 26.2M    0     0  62.4M      0 --:--:-- --:--:-- --:--:--     0 --:--:-- --:--:-- 63.3M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/'\n",
    "!curl 'https://arxiv.org/pdf/2312.11805' -o \"data/gemini.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870e5d3-963c-4001-9229-9695ff8ccbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-d6ce-4b2f-a83d-4973106715a5\n"
     ]
    }
   ],
   "source": [
    "# OPTION: use LlamaParse\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(result_type=\"text\")\n",
    "docs = parser.load_data(\"data/gemini.pdf\")\n",
    "\n",
    "# # OPTION: use SimpleDirectoryReader (uses open-source PyPDF)\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_files=[\"data/llama2.pdf\"])\n",
    "# docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a26ff-6828-413e-b728-2e523632d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage_gemini\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage_gemini\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage_gemini\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b9a68-cb80-4114-bc97-73449d2651a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tavily web search\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "\n",
    "# TODO: remove\n",
    "tavily_api_key = \"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\"\n",
    "tavily_tool = TavilyToolSpec(api_key=tavily_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e13c1c-fa00-437b-ae2e-ca142bbfaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8fd6f-bf2c-47a6-8ed6-d8375aee7268",
   "metadata": {},
   "source": [
    "## Define Agent Services\n",
    "\n",
    "Here we define three services:\n",
    "- Another initial RAG service that will return retrieved nodes, as well as how relevant they are to the question.\n",
    "- A separate web search service that is triggered if there are any irrelevant nodes. Will perform query transformation and web search.\n",
    "- A final summarization service that takes in a set of documents and returns a final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115713b-b387-4b32-8ccb-71a25c2452c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import ComponentService, ServiceComponent, SimpleMessageQueue\n",
    "\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "\n",
    "def to_service_component(component, message_queue, service_name, description):\n",
    "    server = ComponentService(\n",
    "        component=component,\n",
    "        message_queue=message_queue,\n",
    "        description=description,\n",
    "        service_name=service_name,\n",
    "    )\n",
    "    service_component = ServiceComponent.from_component_service(server)\n",
    "    return service_component, server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30f366-81f1-4388-a6d0-17078fe6f6fb",
   "metadata": {},
   "source": [
    "#### Setup Initial RAG Service\n",
    "\n",
    "Runs retrieval and relevancy check on retrieved nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a38608-a6b2-4a38-be37-43312e7faf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "relevancy_prompt_tmpl = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "relevancy_qp = QueryPipeline(chain=[relevancy_prompt_tmpl, llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c2619-8b68-4206-a6b9-f5d7b185ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RAG agent\n",
    "from llama_index.core.query_pipeline import FnComponent\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def run_retrieval(input_str: str) -> Dict:\n",
    "    \"\"\"Run Retrieval.\"\"\"\n",
    "    # retrieves a set of nodes\n",
    "    retrieved_nodes = retriever.retrieve(input_str)\n",
    "\n",
    "    # runs a relevancy check\n",
    "    relevancy_results = []\n",
    "    for node in retrieved_nodes:\n",
    "        relevancy = relevancy_qp.run(context_str=node.text, query_str=query_str)\n",
    "        relevancy_results.append(relevancy.message.content.lower().strip())\n",
    "    contains_irrelevant = \"no\" in relevancy_results\n",
    "\n",
    "    # get relevant texts\n",
    "    relevant_texts = [\n",
    "        retrieved_nodes[i].text\n",
    "        for i, result in enumerate(relevancy_results)\n",
    "        if result == \"yes\"\n",
    "    ]\n",
    "    relevant_text = \"\\n\".join(relevant_texts)\n",
    "\n",
    "    # returns a dictionary of items\n",
    "    return {\n",
    "        \"relevant_text\": relevant_text,\n",
    "        \"contains_irrelevant\": contains_irrelevant,\n",
    "        \"input_str\": input_str,\n",
    "    }\n",
    "\n",
    "\n",
    "retrieval_component = FnComponent(fn=run_retrieval)\n",
    "retrieval_component_s, retrieval_server = to_service_component(\n",
    "    retrieval_component,\n",
    "    message_queue,\n",
    "    \"Runs a retrieval + relevancy check\",\n",
    "    \"retrieval_service\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3faab08-2e1a-49d8-8bbd-3c83037b5609",
   "metadata": {},
   "source": [
    "#### Setup Web Search Service\n",
    "\n",
    "Wrap Tavily into a component that performs query transformation and web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2069c-d5fb-4dbf-a2ec-7635b5038a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "query_transform_tmpl = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n",
    "query_transform_qp = QueryPipeline(chain=[query_transform_tmpl, llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7763a29-b45e-4c59-b54d-051d1743b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_web_search(input_str: str) -> str:\n",
    "    \"\"\"Run Web Search.\"\"\"\n",
    "\n",
    "    transformed_query_str = query_transform_qp.run(query_str=input_str).message.content\n",
    "    # Conduct a search with the transformed query string and collect the results.\n",
    "    search_results = tavily_tool.search(query_str, max_results=5)\n",
    "    return \"\\n\".join([result.text for result in search_results])\n",
    "\n",
    "\n",
    "web_search_component = FnComponent(fn=run_web_search)\n",
    "web_search_component_s, web_server = to_service_component(\n",
    "    web_search_component, message_queue, \"Runs web search\", \"web_search_service\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b94b3-23d4-4604-9676-549a37240c18",
   "metadata": {},
   "source": [
    "### Setup Summarization Service\n",
    "\n",
    "At the end, setup a summarization service that can take in relevant information and join it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d980a00-1e99-43fb-b25c-443957b85795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import SummaryIndex\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def run_summarization(retrieved_text: str, search_text: Optional[str] = None) -> str:\n",
    "    \"\"\"Run summarization.\"\"\"\n",
    "    # use summary index to perform summarization\n",
    "    search_text = search_text or \"\"\n",
    "    documents = [Document(text=retrieved_text + \"\\n\" + search_text)]\n",
    "    index = SummaryIndex.from_documents(documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "    return str(query_engine.query(query_str))\n",
    "\n",
    "\n",
    "summary_component = FnComponent(fn=run_summarization)\n",
    "summary_component_s, summary_server = to_service_component(\n",
    "    summary_component, message_queue, \"Run summarization\", \"summarization_service\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df8fbe-90b0-4625-9d42-e4bdedff909f",
   "metadata": {},
   "source": [
    "## Launch Agent Services\n",
    "\n",
    "Now that we've setup the main components, we can orchestrate them via our pipeline orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f2bf2-657c-46c6-bf13-bdc37455b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    ComponentService,\n",
    ")\n",
    "from llama_index.core.query_pipeline import Link, InputComponent\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    module_dict={\n",
    "        \"input\": InputComponent(),\n",
    "        \"retrieval_server\": retrieval_component_s,\n",
    "        \"web_server\": web_search_component_s,\n",
    "        # TODO: clean this interface up\n",
    "        \"summary_server_no_web\": summary_component_s,\n",
    "        \"summary_server_web\": summary_component_s,\n",
    "    }\n",
    ")\n",
    "pipeline.add_link(\"input\", \"retrieval_server\")\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\",\n",
    "    \"web_server\",\n",
    "    condition_fn=lambda x: x[\"contains_irrelevant\"],\n",
    "    input_fn=lambda x: x[\"input_str\"],\n",
    ")\n",
    "# if web search is called\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\",\n",
    "    \"summary_server_web\",\n",
    "    dest_key=\"retrieved_text\",\n",
    "    condition_fn=lambda x: x[\"contains_irrelevant\"],\n",
    "    input_fn=lambda x: x[\"relevant_text\"],\n",
    ")\n",
    "pipeline.add_link(\"web_server\", \"summary_server_web\", dest_key=\"search_text\")\n",
    "\n",
    "# if web search is not called\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\",\n",
    "    \"summary_server_no_web\",\n",
    "    dest_key=\"retrieved_text\",\n",
    "    condition_fn=lambda x: not x[\"contains_irrelevant\"],\n",
    "    input_fn=lambda x: x[\"relevant_text\"],\n",
    ")\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=pipeline_orchestrator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4a92d-e53c-4682-a12a-e1a03e1990cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents.launchers import LocalLauncher\n",
    "\n",
    "## Define Launcher\n",
    "launcher = LocalLauncher(\n",
    "    [retrieval_server, web_server, summary_server],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfeb648-5cc8-4213-b0bd-f548397da05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_agents.message_queues.simple - Consumer ComponentService-2671543e-633b-492d-a871-0712fbe4ee91: Runs a retrieval + relevancy check has been registered.\n",
      "INFO:llama_agents.message_queues.simple - Consumer ComponentService-e68532cc-d06c-4d77-bd2a-d18cf13df618: Runs web search has been registered.\n",
      "INFO:llama_agents.message_queues.simple - Consumer ComponentService-c68e28ba-73d6-46c1-801b-e60f8ce9fe8e: Run summarization has been registered.\n",
      "INFO:llama_agents.message_queues.simple - Consumer c541b9d2-92b6-4fe5-81d4-2d99a3cb81fa: human has been registered.\n",
      "INFO:llama_agents.message_queues.simple - Consumer ControlPlaneServer-b205f278-fa5b-44cc-8150-e9f07b168250: control_plane has been registered.\n",
      "INFO:llama_agents.services.component - Runs a retrieval + relevancy check launch_local\n",
      "INFO:llama_agents.services.component - Runs web search launch_local\n",
      "INFO:llama_agents.services.component - Run summarization launch_local\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'control_plane' with action 'new_task'\n",
      "INFO:llama_agents.message_queues.simple - Launching message queue locally\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'Runs a retrieval + relevancy check' with action 'new_task'\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'control_plane' to consumer.\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'Runs a retrieval + relevancy check' to consumer.\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'control_plane' with action 'completed_task'\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'Run summarization' with action 'new_task'\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'control_plane' to consumer.\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'Run summarization' to consumer.\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'control_plane' with action 'completed_task'\n",
      "INFO:llama_agents.message_queues.base - Publishing message to 'human' with action 'completed_task'\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'control_plane' to consumer.\n",
      "INFO:llama_agents.message_queues.simple - Successfully published message 'human' to consumer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-training datasets used for Gemini models are multimodal and multilingual, incorporating data from web documents, books, and code. These datasets include image, audio, and video data. The SentencePiece tokenizer is utilized, and training the tokenizer on a large sample of the entire training corpus enhances the inferred vocabulary and subsequently boosts model performance. The models are trained on a diverse range of data sources to improve performance and efficiency across various domains.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Tell me about the pretraining datasets used.\"\n",
    "result = launcher.launch_single(query_str)\n",
    "print(str(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentfile",
   "language": "python",
   "name": "agentfile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
