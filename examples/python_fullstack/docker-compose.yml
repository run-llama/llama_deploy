services:
  qdrant:
    # Used by the RAG workflow
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage

  redis:
    # LlamaDeploy message queue
    image: redis:latest
    hostname: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: redis-cli --raw incr ping
      interval: 5s
      timeout: 3s
      retries: 5

  apiserver:
    # LlamaDeploy API server, will run the workflows
    image: llamaindex/llama-deploy:main
    environment:
      QDRANT_HOST: qdrant
      OPENAI_API_KEY: $OPENAI_API_KEY
    ports:
      - "4501:4501"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: llamactl status
      interval: 5s
      timeout: 3s
      retries: 5
    volumes:
      - ./:/opt/app
    working_dir: /opt/app

  deploy_workflows:
    # Init container, it deploys python_fullstack.yaml and exits
    image: llamaindex/llama-deploy:main
    volumes:
      - ./python_fullstack.yaml:/opt/python_fullstack.yaml
    working_dir: /opt/
    depends_on:
      apiserver:
        condition: service_healthy
    entrypoint: llamactl -s http://apiserver:4501 -t 60 deploy python_fullstack.yaml

  frontend:
    # UI for this deployment, running at http://localhost:3000
    environment:
      APISERVER_URL: http://apiserver:4501
      DEPLOYMENT_NAME: MyDeployment
    build:
      context: ./frontend
      dockerfile: dockerfile
    ports:
      - "3000:3000"
      - "9000:9000"
    volumes:
      - ./frontend:/app
    depends_on:
      deploy_workflows:
        condition: service_completed_successfully

volumes:
  qdrant_data:
