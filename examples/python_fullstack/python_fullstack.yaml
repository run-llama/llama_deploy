name: MyDeployment

control-plane:
  port: 8000

message-queue:
  type: redis
  host: localhost
  port: 6379

default-service: agentic_workflow

services:
  agentic_workflow:
    # A python workflow available in a git repo
    name: Agentic Workflow
    source:
      type: git
      name: https://github.com/run-llama/llama_deploy.git@massi/example-refact
    path: examples/python_fullstack/workflows:agentic_w
    python-dependencies:
      - llama-index-postprocessor-rankgpt-rerank>=0.2.0
      - llama-index-vector-stores-qdrant>=0.3.0
      - llama-index-llms-openai>=0.2.2
      - llama-index-embeddings-openai>=0.2.4
      - llama-index-readers-file>=0.2.0

  rag_workflow:
    # A python workflow available in a git repo
    name: RAG Workflow
    source:
      type: git
      name: https://github.com/run-llama/llama_deploy.git@massi/example-refact
    path: examples/python_fullstack/workflows:rag_w
    python-dependencies:
      - llama-index-postprocessor-rankgpt-rerank>=0.2.0
      - llama-index-vector-stores-qdrant>=0.3.0
      - llama-index-llms-openai>=0.2.2
      - llama-index-embeddings-openai>=0.2.4
      - llama-index-readers-file>=0.2.0
